{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4fc465-765f-4641-9b86-a36e1ff91a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Crisp Common\n",
    "\n",
    "This notebook contains shared code used across all Crisp Blueprints notebooks. Itâ€™s meant to be imported into other notebooks, not run on its own. The goal is to avoid code duplication and simplify maintenance. We chose not to distribute it as a Python package to ensure it's accessible in any environment and user-friendly.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "Make sure you have the following variables set in your environment:\n",
    "\n",
    "- `ACCOUNT_ID`: Your Crisp account ID\n",
    "- `CONNECTOR_ID`: Your Crisp connector ID if using Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efecc642-6a3f-410e-982a-c7a2af277b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## Detect environment that you are running with"
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb76d56-9fd5-43e9-950f-965ce49a7067",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class EnvironmentType(Enum):\n",
    "    COLAB = \"colab\"\n",
    "    DATABRICKS = \"databricks\"\n",
    "    LOCAL = \"local\"\n",
    "\n",
    "\n",
    "environment_type = None\n",
    "ipython_env = str(get_ipython())\n",
    "if \"google.colab\" in ipython_env:\n",
    "    environment_type = EnvironmentType.COLAB\n",
    "elif \"Databricks\" in ipython_env:\n",
    "    environment_type = EnvironmentType.DATABRICKS\n",
    "elif \"ipykernel\" in ipython_env:\n",
    "    environment_type = EnvironmentType.LOCAL\n",
    "else:\n",
    "    raise ValueError(\"Unsupported environment\")\n",
    "\n",
    "print(\"Environment type: {}\".format(environment_type.value))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Install dependencies\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%pip install \\\n",
    "\"tornado==6.4.1\" \\\n",
    "\"pandas>=2.2.2,<3.0.0\" \\\n",
    "\"matplotlib==3.9.1\" \\\n",
    "\"scikit-learn>=1.5.1,<2.0.0\" \\\n",
    "\"seaborn>=0.13.2,<0.14.0\" \\\n",
    "\"plotly>=5.23.0,<6.0.0\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Install environment-specific dependencies"
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b875a8ed-b79b-4cfc-910e-3e53b354fab8",
     "showTitle": false,
     "title": ""
    }
   },
   "cell_type": "code",
   "source": [
    "if environment_type == EnvironmentType.COLAB or environment_type == EnvironmentType.LOCAL:\n",
    "    %pip install \"google-cloud-storage==2.18.0\" \\\n",
    "            \"google-cloud-bigquery[pandas,pyarrow]==3.25.0\" \\\n",
    "            \"google-cloud-bigquery-storage>=2.25.0,<3.0.0\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e947206f-1712-4bc7-8490-679d1aefe4ab",
     "showTitle": false,
     "title": ""
    }
   },
   "cell_type": "markdown",
   "source": [
    "#### Restart Python\n",
    "\n",
    "Colab notebook requires a restart after installing new packages."
   ]
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5842a34-f7a4-46d2-a8fa-bad2357b1491",
     "showTitle": false,
     "title": ""
    }
   },
   "cell_type": "code",
   "source": [
    "if environment_type == EnvironmentType.COLAB:\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e13afd-6bd1-4973-9b9d-14ec70d7e5c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": "## Import dependencies"
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e6adb2-5585-418a-8430-5461f4800e0b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "if environment_type == EnvironmentType.COLAB:\n",
    "    from google.cloud import bigquery, exceptions\n",
    "    from google.colab import auth\n",
    "elif environment_type == EnvironmentType.DATABRICKS:\n",
    "    from pyspark.sql import SparkSession\n",
    "elif environment_type == EnvironmentType.LOCAL:\n",
    "    from google.cloud import bigquery, exceptions\n",
    "else:\n",
    "    print(\"No extra imports\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Set up environment variables"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "os.environ['GRPC_VERBOSITY'] = 'ERROR'\n",
    "os.environ['TK_SILENCE_DEPRECATION'] = '1'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Authenticate (if needed)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if environment_type == EnvironmentType.COLAB:\n",
    "    auth.authenticate_user()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define utility functions"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def eval_python_expression(match):\n",
    "    expression = match.group(1)\n",
    "    global_vars = globals()\n",
    "    return str(eval(expression, global_vars))"
   ]
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "506471e0-9915-4113-9864-85c0555c52e2",
     "showTitle": false,
     "title": ""
    }
   },
   "cell_type": "markdown",
   "source": [
    "## Define magic loading data into a DataFrame\n",
    "\n",
    "The magic cell accepts the query as the cell input and the dataframe name as the argument. The query can contain variables that are defined in the global scope.\n",
    "\n",
    "Example usage:\n",
    "```\n",
    "%%load df\n",
    "SELECT * FROM `{project}.{dataset}.table`\n",
    "```"
   ]
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f8acc1-bb6a-4551-8ab5-6b9ac07e45a1",
     "showTitle": false,
     "title": ""
    }
   },
   "cell_type": "code",
   "source": [
    "@register_cell_magic\n",
    "def load(line, cell):\n",
    "    formatted_query = re.sub(r'\\{(.*?)\\}', eval_python_expression, cell)\n",
    "\n",
    "    if environment_type == EnvironmentType.COLAB:\n",
    "        client = bigquery.Client(project=project)\n",
    "        query_job = client.query(formatted_query)\n",
    "        df = query_job.result().to_dataframe()\n",
    "    elif environment_type == EnvironmentType.LOCAL:\n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(formatted_query)\n",
    "        df = query_job.result().to_dataframe()\n",
    "    elif environment_type == EnvironmentType.DATABRICKS:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.sql(formatted_query).toPandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported environment\")\n",
    "\n",
    "    if line:\n",
    "        globals()[line.strip()] = df\n",
    "    else:\n",
    "        return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Define magic saving a query or Dataframew as a table\n",
    "\n",
    "The magic cell accepts a table name as the argument and a Dataframe or query as the cell input. The table name should be in the format `project.dataset.table`.\n",
    "\n",
    "Example usage:\n",
    "```\n",
    "%%save project.dataset.table \n",
    "SELECT * FROM `{project}.{dataset}.table`\n",
    "```\n",
    "```\n",
    "%%save project.dataset.table\n",
    "df\n",
    "```"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@register_cell_magic\n",
    "def save(line, cell):\n",
    "    global_vars = globals()\n",
    "\n",
    "    input_first_line = cell.strip().split(\"\\n\")[0]\n",
    "    try:\n",
    "        df = global_vars[input_first_line]\n",
    "        is_dataframe = isinstance(df, pd.DataFrame)\n",
    "    except KeyError:\n",
    "        is_dataframe = False\n",
    "\n",
    "    table_id = re.sub(r'\\{(.*?)\\}', eval_python_expression, line.strip())\n",
    "\n",
    "    table = table_id.split(\".\")\n",
    "\n",
    "    if len(table) != 3:\n",
    "        raise ValueError(\"Table name should be in the format project.dataset.table\")\n",
    "    project, dataset, table = table\n",
    "\n",
    "    if environment_type == EnvironmentType.COLAB or environment_type == EnvironmentType.LOCAL:\n",
    "        client = bigquery.Client(project=project)\n",
    "        dest_dataset = client.dataset(project=project, dataset_id=dataset)\n",
    "        try:\n",
    "            dest_dataset = client.get_dataset(dest_dataset)\n",
    "        except exceptions.NotFound:\n",
    "            dest_dataset = client.create_dataset(dest_dataset)\n",
    "\n",
    "        if is_dataframe:\n",
    "            job_config = bigquery.LoadJobConfig()\n",
    "            job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "            table_ref = dest_dataset.table(table)\n",
    "            job = client.load_table_from_dataframe(df, destination=table_ref, job_config=job_config)\n",
    "            job.result()\n",
    "        else:\n",
    "            formatted_query = cell.format(**global_vars)\n",
    "            job_config = bigquery.QueryJobConfig(destination=dest_dataset.table(table),\n",
    "                                                 write_disposition=\"WRITE_TRUNCATE\")\n",
    "            query_job = client.query(formatted_query, job_config=job_config)\n",
    "            query_job.result()\n",
    "\n",
    "    elif environment_type == EnvironmentType.DATABRICKS:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        if is_dataframe:\n",
    "            spark.createDataFrame(df).write.saveAsTable(table)\n",
    "        else:\n",
    "            formatted_query = cell.format(**global_vars)\n",
    "            spark.sql(formatted_query).write.saveAsTable(table)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported environment\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb22533-244e-47fa-bb6f-a04e544e4602",
     "showTitle": false,
     "title": ""
    }
   },
   "cell_type": "markdown",
   "source": "### Define source dataset"
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201aed74-4e59-40fa-9c75-273ba2513af2",
     "showTitle": false,
     "title": ""
    }
   },
   "cell_type": "code",
   "source": [
    "account_id = os.getenv('ACCOUNT_ID')\n",
    "\n",
    "if not account_id or account_id == \"[your-account-id]\":\n",
    "    raise ValueError(\"Please set your ACCOUNT_ID\")\n",
    "\n",
    "if environment_type == EnvironmentType.COLAB or environment_type == EnvironmentType.LOCAL:\n",
    "    project = 'crisp-frontier-dev'\n",
    "    dataset = f\"analytics_blueprints_{account_id}\"\n",
    "elif environment_type == \"databricks\":\n",
    "    project = 'prod'\n",
    "    connector_id = os.getenv('CONNECTOR_ID')\n",
    "    if not connector_id or connector_id == \"[your-connector-id]\":\n",
    "        raise ValueError(\"Please set your CONNECTOR_ID\")\n",
    "    dataset = f\"schema_{account_id}_{connector_id}\"\n",
    "else:\n",
    "    print(\"Unsupported environment\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "crisp_basics",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
