{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d4fc465-765f-4641-9b86-a36e1ff91a3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![Crisp](img/logo.png)\n",
    "# Crisp Common\n",
    "\n",
    "This notebook contains shared code used across all Crisp Blueprints notebooks. It’s meant to be imported into other notebooks, not run on its own. The goal is to avoid code duplication and simplify maintenance. We chose not to distribute it as a Python package to ensure it's accessible in any environment and user-friendly.\n",
    "\n",
    "## Pre-requisites\n",
    "\n",
    "Make sure you have the following variables set in your environment:\n",
    "\n",
    "- `ACCOUNT_ID`: Your Crisp account ID\n",
    "- `CONNECTOR_ID`: Your Crisp connector ID if using Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efecc642-6a3f-410e-982a-c7a2af277b5f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Detect environment that you are running with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fb76d56-9fd5-43e9-950f-965ce49a7067",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class EnvironmentType(Enum):\n",
    "    COLAB = \"colab\"\n",
    "    DATABRICKS = \"databricks\"\n",
    "    LOCAL = \"local\"\n",
    "\n",
    "\n",
    "environment_type = None\n",
    "ipython_env = str(get_ipython())\n",
    "if \"google.colab\" in ipython_env:\n",
    "    environment_type = EnvironmentType.COLAB\n",
    "elif \"Databricks\" in ipython_env:\n",
    "    environment_type = EnvironmentType.DATABRICKS\n",
    "elif \"ipykernel\" in ipython_env:\n",
    "    environment_type = EnvironmentType.LOCAL\n",
    "else:\n",
    "    raise ValueError(\"Unsupported environment\")\n",
    "\n",
    "print(\"Environment type: {}\".format(environment_type.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \\\n",
    "\"tornado==6.4.1\" \\\n",
    "\"pandas>=2.2.2,<3.0.0\" \\\n",
    "\"matplotlib==3.9.1\" \\\n",
    "\"scikit-learn>=1.5.1,<2.0.0\" \\\n",
    "\"seaborn>=0.13.2,<0.14.0\" \\\n",
    "\"plotly>=5.23.0,<6.0.0\" \\\n",
    "\"openai>=1.44.1\" \\\n",
    "\"langchain-openai>=0.2.1\" \\\n",
    "\"folium>=0.17.0\" \\\n",
    "\"ipywidgets>=7,<8\" \\\n",
    "\"prophet==1.1.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install environment-specific dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b875a8ed-b79b-4cfc-910e-3e53b354fab8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if environment_type == EnvironmentType.COLAB or environment_type == EnvironmentType.LOCAL:\n",
    "    %pip install \"google-cloud-storage==2.18.0\" \\\n",
    "            \"google-cloud-bigquery[pandas,pyarrow]==3.25.0\" \\\n",
    "            \"google-cloud-bigquery-storage>=2.25.0,<3.0.0\"\n",
    "\n",
    "if environment_type == EnvironmentType.LOCAL:\n",
    "    %pip install \"python-dotenv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e13afd-6bd1-4973-9b9d-14ec70d7e5c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e6adb2-5585-418a-8430-5461f4800e0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "if environment_type == EnvironmentType.COLAB:\n",
    "    from google.cloud import bigquery, exceptions\n",
    "    from google.colab import auth\n",
    "elif environment_type == EnvironmentType.DATABRICKS:\n",
    "    from pyspark.sql import SparkSession\n",
    "elif environment_type == EnvironmentType.LOCAL:\n",
    "    from google.cloud import bigquery, exceptions\n",
    "else:\n",
    "    print(\"No extra imports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GRPC_VERBOSITY\"] = \"ERROR\"\n",
    "os.environ[\"TK_SILENCE_DEPRECATION\"] = \"1\"\n",
    "\n",
    "if environment_type == EnvironmentType.LOCAL:\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "    denv = find_dotenv(raise_error_if_not_found=False, usecwd=True)\n",
    "    l = load_dotenv(denv)\n",
    "    if l:\n",
    "        print(\"Loaded .env file from {}\".format(denv))\n",
    "    else:\n",
    "        print(\"No .env file found in {}\".format(denv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authenticate (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if environment_type == EnvironmentType.COLAB:\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_python_expression(match):\n",
    "    expression = match.group(1)\n",
    "    global_vars = globals()\n",
    "    return str(eval(expression, global_vars))\n",
    "\n",
    "\n",
    "def transform_sql_to_databricks(sql: str) -> str:\n",
    "    transformations = [\n",
    "        # Rule for DATE_TRUNC -> TRUNC\n",
    "        (\n",
    "            r\"DATE_TRUNC\\(DATE\\((?P<column>.*?)\\),\\s*(?P<time_unit>\\w+)\\)\",\n",
    "            \"TRUNC(DATE({column}), '{time_unit}')\",\n",
    "            {},\n",
    "        ),\n",
    "        (\n",
    "            r\"DATE_SUB\\((?P<function>.*?),\\s*INTERVAL\\s*(?P<value>\\d+)\\s*DAY\\)\",\n",
    "            \"DATE_SUB({function}, {value})\",\n",
    "            {},\n",
    "        ),\n",
    "        # Add more transformations here as needed\n",
    "        # 1) Weekly truncation: DATE_TRUNC(CAST(... AS DATE) or DATE(...), WEEK(MONDAY))\n",
    "        (\n",
    "            r\"DATE_TRUNC\\(\\s*(?:DATE\\((?P<col1>.*?)\\)|CAST\\((?P<col2>.*?)\\s+AS\\s+DATE\\))\\s*,\\s*WEEK(?:\\(\\s*MONDAY\\s*\\))?\\s*\\)\",\n",
    "            \"date_trunc('week', cast({column} as timestamp))\",\n",
    "            {\"column\": lambda gd: gd.get('col1') or gd.get('col2')},\n",
    "        ),\n",
    "        # 1b) Other units: DATE_TRUNC(DATE(...)/CAST(... AS DATE), UNIT)\n",
    "        (\n",
    "            r\"DATE_TRUNC\\(\\s*(?:DATE\\((?P<col>.*?)\\)|CAST\\((?P<col2>.*?)\\s+AS\\s+DATE\\))\\s*,\\s*(?P<unit>\\w+)\\s*\\)\",\n",
    "            \"date_trunc('{time_unit}', cast({column} as timestamp))\",\n",
    "            {\n",
    "                \"column\": lambda gd: gd.get('col') or gd.get('col2'),\n",
    "                \"time_unit\": lambda gd: gd.get('unit').lower(),\n",
    "            },\n",
    "        ),\n",
    "        # 2) DATE_ADD  → date_add (handles parentheses and arithmetic in interval)\n",
    "        (\n",
    "            r\"DATE_ADD\\(\\s*(?P<func>[^,]+?)\\s*,\\s*INTERVAL\\s*\\(?\\s*(?P<val>[^)]+?)\\s*\\)?\\s*DAY\\s*\\)\",\n",
    "            \"date_add({func}, {val})\",\n",
    "            {},\n",
    "        ),\n",
    "        # 3) DATE_SUB  → date_sub (handles parentheses and arithmetic in interval)\n",
    "        (\n",
    "            r\"DATE_SUB\\(\\s*(?P<function>[^,]+?)\\s*,\\s*INTERVAL\\s*\\(?\\s*(?P<value>[^)]+?)\\s*\\)?\\s*DAY\\s*\\)\",\n",
    "            \"date_sub({function}, {value})\",\n",
    "            {},\n",
    "        ),\n",
    "        # 4a) CAST AS FLOAT64 → CAST AS DOUBLE\n",
    "        (\n",
    "            r\"CAST\\(\\s*(?P<expr>.*?)\\s+AS\\s+FLOAT64\\s*\\)\",\n",
    "            \"CAST({expr} AS DOUBLE)\",\n",
    "            {},\n",
    "        ),\n",
    "        # 4b) CAST AS DATETIME → CAST AS TIMESTAMP\n",
    "        (\n",
    "            r\"CAST\\(\\s*(?P<expr>.*?)\\s+AS\\s+DATETIME\\s*\\)\",\n",
    "            \"CAST({expr} AS TIMESTAMP)\",\n",
    "            {},\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    for pattern, replacement, abstracted_components in transformations:\n",
    "        def replacer(match):\n",
    "            gd = match.groupdict()\n",
    "            # resolve any callable components\n",
    "            comps = {}\n",
    "            for k, v in abstracted_components.items():\n",
    "                comps[k] = v(gd) if callable(v) else v\n",
    "            return replacement.format(**gd, **comps)\n",
    "\n",
    "        sql = re.sub(pattern, replacer, sql, flags=re.IGNORECASE)\n",
    "\n",
    "    return sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "506471e0-9915-4113-9864-85c0555c52e2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define magic loading data into a DataFrame\n",
    "\n",
    "The magic cell accepts the query as the cell input and the dataframe name as the argument. The query can contain variables that are defined in the global scope.\n",
    "\n",
    "Example usage:\n",
    "```\n",
    "%%load df\n",
    "SELECT * FROM `{project}`.`{dataset}`.`table`\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f8acc1-bb6a-4551-8ab5-6b9ac07e45a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def load(line, cell):\n",
    "    formatted_query = re.sub(r\"\\{(.*?)\\}\", eval_python_expression, cell)\n",
    "\n",
    "    if environment_type == EnvironmentType.COLAB:\n",
    "        client = bigquery.Client(project=project)\n",
    "        query_job = client.query(formatted_query)\n",
    "        df = query_job.result().to_dataframe()\n",
    "    elif environment_type == EnvironmentType.LOCAL:\n",
    "        client = bigquery.Client()\n",
    "        query_job = client.query(formatted_query)\n",
    "        df = query_job.result().to_dataframe()\n",
    "    elif environment_type == EnvironmentType.DATABRICKS:\n",
    "        formatted_query = re.sub(\n",
    "            r\"`exp_\", \"`\", transform_sql_to_databricks(formatted_query)\n",
    "        )  # Exported tables have that prefix dropped\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        df = spark.sql(formatted_query).toPandas()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported environment\")\n",
    "\n",
    "    if line:\n",
    "        globals()[line.strip()] = df\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define magic saving a query or Dataframew as a table\n",
    "\n",
    "The magic cell accepts a table name as the argument and a Dataframe or query as the cell input. The table name should be in the format `project.dataset.table`.\n",
    "\n",
    "Example usage:\n",
    "```\n",
    "%%save project.dataset.table \n",
    "SELECT * FROM `{project}`.`{dataset}`.`table`\n",
    "```\n",
    "```\n",
    "%%save project.dataset.table\n",
    "df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def save(line, cell):\n",
    "    global_vars = globals()\n",
    "\n",
    "    input_first_line = cell.strip().split(\"\\n\")[0]\n",
    "    try:\n",
    "        df = global_vars[input_first_line]\n",
    "        is_dataframe = isinstance(df, pd.DataFrame)\n",
    "    except KeyError:\n",
    "        is_dataframe = False\n",
    "\n",
    "    table_id = re.sub(r\"\\{(.*?)\\}\", eval_python_expression, line.strip())\n",
    "\n",
    "    table = table_id.split(\".\")\n",
    "\n",
    "    if len(table) != 3:\n",
    "        raise ValueError(\"Table name should be in the format project.dataset.table\")\n",
    "    project, dataset, table = table\n",
    "\n",
    "    if (\n",
    "        environment_type == EnvironmentType.COLAB\n",
    "        or environment_type == EnvironmentType.LOCAL\n",
    "    ):\n",
    "        client = bigquery.Client(project=project)\n",
    "        dest_dataset = client.dataset(project=project, dataset_id=dataset)\n",
    "        try:\n",
    "            dest_dataset = client.get_dataset(dest_dataset)\n",
    "        except exceptions.NotFound:\n",
    "            dest_dataset = client.create_dataset(dest_dataset)\n",
    "\n",
    "        if is_dataframe:\n",
    "            job_config = bigquery.LoadJobConfig()\n",
    "            job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "            table_ref = dest_dataset.table(table)\n",
    "            job = client.load_table_from_dataframe(\n",
    "                df, destination=table_ref, job_config=job_config\n",
    "            )\n",
    "            job.result()\n",
    "        else:\n",
    "            formatted_query = cell.format(**global_vars)\n",
    "            job_config = bigquery.QueryJobConfig(\n",
    "                destination=dest_dataset.table(table),\n",
    "                write_disposition=\"WRITE_TRUNCATE\",\n",
    "            )\n",
    "            query_job = client.query(formatted_query, job_config=job_config)\n",
    "            query_job.result()\n",
    "\n",
    "    elif environment_type == EnvironmentType.DATABRICKS:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "        if is_dataframe:\n",
    "            frame = spark.createDataFrame(df)\n",
    "        else:\n",
    "            formatted_query = cell.format(**global_vars)\n",
    "            formatted_query = re.sub(\n",
    "                r\"`exp_\", \"`\", transform_sql_to_databricks(formatted_query)\n",
    "            )\n",
    "            frame = spark.sql(formatted_query)\n",
    "        frame.write.mode(\"overwrite\").saveAsTable(table_id)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fb22533-244e-47fa-bb6f-a04e544e4602",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Define source dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201aed74-4e59-40fa-9c75-273ba2513af2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "account_id = os.getenv(\"ACCOUNT_ID\")\n",
    "\n",
    "if not account_id or account_id == \"[your-account-id]\":\n",
    "    raise ValueError(\"Please set your ACCOUNT_ID\")\n",
    "\n",
    "if (\n",
    "    environment_type == EnvironmentType.COLAB\n",
    "    or environment_type == EnvironmentType.LOCAL\n",
    "):\n",
    "    project = \"customer-support-sandbox\"\n",
    "    dataset = f\"analytics_blueprints_{account_id}\"\n",
    "elif environment_type == EnvironmentType.DATABRICKS:\n",
    "    project = \"prod\"\n",
    "    connector_id = os.getenv(\"CONNECTOR_ID\")\n",
    "    if not connector_id or connector_id == \"[your-connector-id]\":\n",
    "        raise ValueError(\"Please set your CONNECTOR_ID\")\n",
    "    dataset = f\"schema_{account_id}_{connector_id}\"\n",
    "else:\n",
    "    print(\"Unsupported environment\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "crisp_basics",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
